{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e1c888c5",
            "metadata": {},
            "source": [
                "# üêç Ouroboros Colab Inference Server (LLaDA - CPU Offload)\n",
                "\n",
                "This notebook runs LLaDA-8B-Instruct with **CPU offloading** for T4 GPU.\n",
                "\n",
                "### Instructions\n",
                "1.  **Runtime**: Go to `Runtime` -> `Change runtime type` -> Select **T4 GPU**.\n",
                "2.  **Ngrok**: Get your authtoken from [ngrok.com](https://ngrok.com).\n",
                "3.  **Hugging Face**: Have your HF Token ready.\n",
                "4.  **Run**: Execute all cells below **IN ORDER**.\n",
                "5.  **Connect**: Copy the ngrok URL to your local `.env` as `COLAB_API_URL`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24fb631f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Install Dependencies\n",
                "!pip install -q transformers==4.38.2 pyngrok fastapi uvicorn nest-asyncio torch accelerate huggingface_hub bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4b0e0a1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Clone LLaDA Repository\n",
                "import os\n",
                "import sys\n",
                "\n",
                "if not os.path.exists('/content/LLaDA'):\n",
                "    !git clone https://github.com/ML-GSAI/LLaDA.git /content/LLaDA\n",
                "    print(\"‚úÖ Cloned LLaDA repository\")\n",
                "else:\n",
                "    print(\"‚úÖ LLaDA repository already exists\")\n",
                "\n",
                "if '/content/LLaDA' not in sys.path:\n",
                "    sys.path.insert(0, '/content/LLaDA')\n",
                "    print(f\"‚úÖ Added to Python path\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "633ed969",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Setup Ngrok\n",
                "import getpass\n",
                "from pyngrok import ngrok, conf\n",
                "\n",
                "print(\"Enter your ngrok authtoken (hidden):\")\n",
                "token = getpass.getpass()\n",
                "conf.get_default().auth_token = token\n",
                "print(\"‚úÖ Ngrok configured!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "516a3893",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Setup Hugging Face\n",
                "import getpass\n",
                "\n",
                "print(\"Enter your Hugging Face Token:\")\n",
                "hf_token = getpass.getpass(\"HF Token: \")\n",
                "\n",
                "if hf_token.strip():\n",
                "    from huggingface_hub import login\n",
                "    login(token=hf_token.strip())\n",
                "    print(\"‚úÖ Logged in!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a365fef5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 5. Load LLaDA with 8-bit + CPU Offload\n",
                "import torch\n",
                "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
                "from generate import generate\n",
                "\n",
                "print(\"‚úÖ Imported official generate()\")\n",
                "\n",
                "MODEL_NAME = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
                "print(f\"\\nLoading {MODEL_NAME} with 8-bit + CPU offload...\")\n",
                "\n",
                "# 8-bit quantization with CPU offload enabled\n",
                "quant_config = BitsAndBytesConfig(\n",
                "    load_in_8bit=True,\n",
                "    llm_int8_enable_fp32_cpu_offload=True  # KEY: Allow CPU offload!\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "\n",
                "# Custom device map: keep most on GPU, offload lm_head to CPU\n",
                "device_map = {\n",
                "    \"model\": 0,      # Main model on GPU\n",
                "    \"lm_head\": \"cpu\" # Offload language model head to CPU\n",
                "}\n",
                "\n",
                "model = AutoModel.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=quant_config,\n",
                "    device_map=device_map,\n",
                "    trust_remote_code=True\n",
                ").eval()\n",
                "\n",
                "print(\"‚úÖ LLaDA loaded successfully (8-bit + CPU offload)!\")\n",
                "print(f\"Model devices: {model.hf_device_map}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "269f4a16",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 6. Start Server\n",
                "import nest_asyncio\n",
                "import uvicorn\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel\n",
                "\n",
                "nest_asyncio.apply()\n",
                "app = FastAPI()\n",
                "\n",
                "class GenerationRequest(BaseModel):\n",
                "    prompt: str\n",
                "    max_tokens: int = 128\n",
                "    temperature: float = 0.0\n",
                "\n",
                "class GenerationResponse(BaseModel):\n",
                "    generated_text: str\n",
                "    tokens_used: int\n",
                "\n",
                "@app.post(\"/generate\")\n",
                "async def generate_text(req: GenerationRequest):\n",
                "    try:\n",
                "        messages = [{\"role\": \"user\", \"content\": req.prompt}]\n",
                "        prompt_text = tokenizer.apply_chat_template(\n",
                "            messages,\n",
                "            add_generation_prompt=True,\n",
                "            tokenize=False\n",
                "        )\n",
                "        \n",
                "        input_ids = tokenizer(prompt_text)['input_ids']\n",
                "        input_ids = torch.tensor(input_ids).to('cuda').unsqueeze(0)\n",
                "        \n",
                "        print(f\"Generating {req.max_tokens} tokens...\")\n",
                "        \n",
                "        output_ids = generate(\n",
                "            model,\n",
                "            input_ids,\n",
                "            steps=req.max_tokens,\n",
                "            gen_length=req.max_tokens,\n",
                "            block_length=32,\n",
                "            temperature=req.temperature,\n",
                "            cfg_scale=0.0,\n",
                "            remasking='low_confidence'\n",
                "        )\n",
                "        \n",
                "        generated_text = tokenizer.batch_decode(\n",
                "            output_ids[:, input_ids.shape[1]:],\n",
                "            skip_special_tokens=True\n",
                "        )[0]\n",
                "        \n",
                "        tokens_used = output_ids.shape[1] - input_ids.shape[1]\n",
                "        print(f\"‚úÖ Generated: {generated_text[:100]}...\")\n",
                "        \n",
                "        return GenerationResponse(\n",
                "            generated_text=generated_text,\n",
                "            tokens_used=tokens_used\n",
                "        )\n",
                "    except Exception as e:\n",
                "        import traceback\n",
                "        error_trace = traceback.format_exc()\n",
                "        print(f\"‚ùå Error: {error_trace}\")\n",
                "        raise HTTPException(status_code=500, detail=f\"{str(e)}\\n{error_trace}\")\n",
                "\n",
                "public_url = ngrok.connect(8000)\n",
                "print(f\"\\nüî• SERVER RUNNING! üî•\")\n",
                "print(f\"Copy this URL to your local .env: {public_url.public_url}\\n\")\n",
                "\n",
                "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
                "server = uvicorn.Server(config)\n",
                "await server.serve()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
